# Re: Monitoring

## A Few types of monitoring systems

### Centralized pollers
The oldest tools we describe as "monitoring tools" are centralized pollers.
This is typically (but not always) a single host that sits somewhere on the
network and, like the unfortunate employee in those cell-provider commercials,
constantly asks the question "can you hear me now" to every host you configure
it to "monitor". If any of those hosts fail to respond, the system sends you a
notification (typically via SMS, but more commonly these days, via an alerting
service like PagerDuty or VictorOps). 

As you can imagine, centralized pollers don't scale terribly well, and
depending on how many hosts you need to monitor, they usually operate on a
1-minute or greater "tick". Nagios, probably the most popular monitoring system
in the world today is a Centralized Poller.

Some centralized pollers use an "Agent", which is a piece of software that
resides on every host you want to monitor. The agent's job is to collect
measurements on a single host and then report them back to the monitoring
server whenever it asks. 

### Passive collectors 
More modern systems often sit passively on the network and wait for updates
from remote agents. They maintain timers on every system they're configured to
"monitor", and if the timer for a given system expires before the system checks
in, the system sends you a notification. Passive collectors always employ
agents, and they generally scale better than Centralized pollers. Many systems
also tow the line, using both active polling and passive collection depending
on how you configure them. Sensu is arguably a Passive collector that works
well and scales much better than it's traditional competitors.

### Roll-up Collectors 
Roll-up patterns are often used in HPC labs where we have thousands of
individual hosts. The idea is to use an agent on every system to collect
measurements, which are then sent to intermediate systems (Imagine a rack of
servers that all collect and send their metrics to the server at the top of
their rack). The intermediate system then "rolls-up" all the measurements it
recieves and provides all of that data in a compressed form to possibly another
intermediate (eg.. the top server in the first rack of the row), or to the
monitoring system itself. Efficient roll-up systems can collect metrics on the
order of seconds from a massive number of individual hosts. A good example of
this pattern in the wild is Ganglia, whose Gmond agent uses multicast to detect
and aggregate metrics between systems in the same cluster.

### Process Emitters/Reporters
In this book, when I say the word "instrumentation" I'm speaking of code that
resides inside the process (literally inside the PID) of the thing you want to
monitor. Instrumentation libraries enable software developers to make
measurements inside their applications, counting the number of times this or
that function is involked and timing interactions with things like database
services and etc.. Instrumentation is the only way to get some kinds of
metrics, like for example the number of threads being involked by your process,
and how long-lived those threads are. 

Instrumentation libraries typically use one of two patterns internally.
Emitters immediatly purge their metrics via a non-blocking channel (usually
inter-process or UDP to a locally listening socket), while Reporters use a
non-blocking thread to hold their metrics in-memory, reporting them only when
asked (usually via a litening network socket. Statsd is a wildly popular
process-emitter while Dropwizard metrics is a great process-reporter. 

### APM
The problem with distributed web architectures is that they're so...
distributed. Your code is slathered from inside the end-users browser all the
way back to the Database (which you've heard is somewhere in Pittsburg).
Application Performance Monitoring, or APM tries to measure the performance
characteristics of a web-application from one end to the other; breaking down
how long every little hunk of code took to do it's thing, so when someone says
"the website is slow" you can hopefully see where you need to go to fix it.

Typically these tools use Bytecode injection and/or Monkey-patching to modify
your code, compiler or interpretor at run-time, wrapping classes and functions
with versions that extract timing information.  Those timing numbers are then
emitted as metrics and sent into the APM's data collection framework. Data
resolution and retention varies widely, though many of these tools work on a
60-second tick, which is perfectly adequate in real life.

In my admittedly limited experience (and no-doubt already tiresome opinion),
APM tools remain effective by retaining their focus on augmenting engineering
know-how and otherwise staying out of the way. Many do this well, and many
others sprawl here and there in an attempt to find novel uses for machine
learning or impress management with world-maps, doughnut-graphs and node/edge
diagrams. Don't get me wrong, many of these features are nifty, and helpful for
non-technical users. That's great, but beware tools that put that stuff first
-- actually making you click through maps to get to timing data or otherwise
attempting to enforce their interpretation of your architecture in the UI in
ways that make you work around it. You're going to want simple line-graphs of
timing data way more often than you're going to want a map, and you're going to
be using the tool more often than non-technical users (assuming it actually
works).

### RUM
RUM, or "Real User Monitoring" used to be a thing before it was swallowed by
APM. The idea is, since you have javascript running inside your user's browser,
you may as well inject a timer or two, just to see how long it took that user
to load your page. RUM is probably the biggest deal in web-performance
monitoring in the last decade or so. It's really helping people who want pages
to load faster close the loop with the people who are working to make those
pages load faster.

There aren't very many RUM-only tools out there (none are represented in this
book) because RUM has largly become a feature of the various APM systems. Bucky
(http://github.hubspot.com/bucky/) is a good open source library if you're just
looking for some stand-alone RUM.

### Exception Tracking

Exception tracking systems count crashes (In polite society we refer to them as
"exceptions"). They usually do this from within web-based applications
server-side, or in code running in your user's browser on the client-side. They
often walk the line between RUM and Log-Processing systems, having neither the
open-endedness of logging systems nor the performance-centric focus of RUM.
Their simplicity and single-mindedness make them cheap, easy to configure,
simple to use, and very reliable. A collection of happy factors that combine to
often make them the go-to application monitoring solution when centralized
logging is too much of a hassle, and APM/RUM is too hulking, confusing,
unreliable and/or expensive. 

These tools generally rely on the exception handling charateristics built-in to
languages like Java and Python; giving you a function to call when exceptions
happen or a function to wrap dangerous code in for languages that implement
error handeling instead. Since the data is a-periodic (hopefully your code only
crashes so often) and finite (there are only so many ways it can crash), these
systems almost always store raw-resolution data to relational data-stores, and
boast simple, effective UI's. If you're thinking about using a JavaScript
exception tracker, you should know that the quality of data they're able to
return will vary wildly with the quality of the browser-side JS interpretor in
use by your customers.

### Remote Polling

Remote pollers use a geographically distributed array of "sensors" (machines
capable of running /bin/ping (I jest (they also run telnet)) to collect
availability data, and sometimes some light performance data from a publicly
facing service like your website or mail server. Many APM tools and commercial
monoliths (Circonus) have their own remote polling networks which they operate
along-side their other offerings, but there are also plenty of commercial
offerings that specialize in remote polling as a service (pingdom, nodeping).

## A Few things you should know about monitoring systems

### Think big, but use small tools.
New monitoring systems spring into being for myriad reasons. The best ones come
from engineers like you who couldn't find the tool they needed to get their
work done. These tools tend to be small, efficient, and purpose-specific. They
tend to get along well with other tools, and they solve more problems than they
create.  Other tools don't do these things. They come from people who were
looking for a market niche who say things like "THIS tool can monitor ANYTHING
with ZERO EFFORT!". 

Monitoring tools excel when they start with a very strong focus and iterate on
it until it's rock-solid.  This book exists today to more or less deliver this
message: beware the tool that claims to do it all, because today, as I write
this, the reality is there probaby isn't one tool that's going to take care of
"monitoring" for your orginazation (unless your orginzation only has one thing
to monitor and one person to monitor it). It's really not a question of
features and "what can be monitored", it's a question of people and trade-offs.

Simple tools have scaling problems. But then again, tools that scale well are
often difficult to maintain.  Tools that are easy to maintain are often
expensive, and cheaper tools tend to become more widely adopted throughout the
organization, and on and on. The combination of tools you'll eventually choose
should have way more to do with the people around you, and things like how much
money and time you have, than how many thingies the monitoring system claims to
be able to watch. A simple tool that answers one question for every engineering
team is worth 1000 do-it-all tools that only help Ops, or the network team.

But even if that weren't true -- if we could ignore the human factors invovled
here, there certainly is no single monitoring tool that has this entire problem
licked, and that's just a laws of physics thing. As you'll see below, if you
need raw-resolution data, you're going to have to store less of it, or foot the
bill for a large stream-processing and storage infrastructure. If you want
flexibility with respect to *how* a tool monitors the things you care about,
you'll have to develop some in-house expertiese to maintain your
customizations. For every advantage there's a trade-off. That's just the way it
is. As my Mom used to say: Them's the breaks kid.

So if you find yourself considering a tool that claims to be the final and
ultimate solution to the monitoring problem, my advice is to assume you're
working with a tool (or more likely a salesperson) that doesn't understand the
problem very well and move on.  You want to be using tools that are savvy to,
and up-front about their limitations. You want to be using tools that
acknowledge the presence of other monitoring tools, and have an
interoperability story to tell about each one.

### Push vs Pull

This is perhaps the monitoring world's analog to Emacs vs Vi, but I will
attempt an objective overview here. Many classic monitoring systems act a lot
like my buddy Eric. They sit on the couch at the center of the party,
constantly asking everyone "Hey, you ok?" and then freaking out when they get a
bad response, or no response at all. These systems are pull-based. Pulling (or
polling) is good because you can't always count on other people to tell you
when they're not Ok, and it also consolodates all the configuration centrally.
You configure a pull-based system with a list of hosts, and it's responsible
for going through that list over and over again, making sure everyone is OK.
That's way simpler than configuring 5000 systems to "report in" every so often.
Right?

But then on the other hand, to get any really *useful* data (CPU utilization,
process lists, etc..) from our systems we're going to have to install an agent
anyway (which really isn't a big deal anymore because configuration management
is a thing now) so while we're at it, we may as well tell the agent to shout
out to the monitoring system every so often. Then if the monitoring system
doesn't hear from one particular host in a while it'll either notify someone or
attempt to manually check-in itself. These are push-based systems, they work
the same way I treat my parents. I assume everything is ok as long as they call
me every so often, but if I don't hear from them in a while, I give them a
ping. Push-based monitoring tools are generally believed to scale better
(though this is a point of contention), since the monitoring system doesn't
need to deal with the overhead of checking up on everyone all the time. It can
spend all of it's time processing results. 

I think it's generally fair to say that right now as I write this, push-based
systems in the real world are usually the more scalable systems. Most SaaS
offerings are push-based, and many older on-premise tools are pull-based. If
you're at a small-to medium sized org with less than 100,000 services to poll,
you'll do fine with a pull-based system, but if you're larger than that, make
sure you understand the scaling model of the monitoring systems you're
considering. 

### Agent vs Agentless
This is another of the classic debates. In the 90's, all monitoring agents were
horrible pieces of software that wreaked all sorts of havock (a few still are
today). Back then, agents were to be avoided at all costs, but these days
monitoring agents are generally pretty well behaved. The systems that use a
good agent are generally to be preferred over agentless systems, which always
need to commit some egregious security no-no's like running under domain-admin
priviledges or centrally storing general-purpose credentials to all your boxes.

Collectd is a solid, efficient, well-tested open-source monitoring agent upon
which many commercial products base their proprietary collector software.
Beware of agentless systems (make sure you understand the security trade-offs
they make), and take it as a good sign if the vendor you're looking at has a
collectd-based agent.

### Data summarization and storage
Time series databases are really hard. Anyone who says otherwise is trying to
sell you something. It really doesn't matter how large you are today or how
much you're trying to measure, you probably can't afford to store
raw-resolution data (unless you can spring for a hadoop infratructure for
metrics alone). So pretty much every TSDB performs some form of data
summarization (also referred to as aggregation or rollups). The idea is to
consolidate the individual data points you collect into fewer datapoints that
accurately summarize the original measurements.

I don't have the room to devle very deeply into this subject but it's been
widely documented and you should read up. Monitoring systems vary widely in how
they accomplish data aggregation, which is relevent to you because you want to
ensure that your data isn't destroyed in the summarization process. On one
extreme are the systems that expose all of the dials and knobs to you, giving
you all the rope you need to hang yourself and walking away, while on the other
are the systems which try to insulate you by blithy destroying your data for
you and assuming you're too stupid or apathetic to care.

Good metrics systems are very clear about their data retention and resolution
schedules. They'll say "we store 1-second resolution data for X days, and then
we agregate it by doing Y". Be careful with systems that don't provide this
information, because they're probably averaging your data into oblivion without
telling you. Most systems will make you classify your inbound metrics,
assigning to them types like "gauge", "counter", or "timer". This is admittedly
boring, but critically important information, so make sure you understand how
the system you choose uses those classifications internally.  Some systems will
automatically assign destructive roll-ups to certain types, always applying
mean-average to consolidate gauge metrics for example.

"How does your system treat gauge metrics versus counter metrics in the
persistence layer?" is a great question to ask the pre-sales engineer sitting
in your conference room.

Finally, several systems employ "data-point capping", which I have mixed
feelings about.  That is, regardless of storage resolution, their UI will only
display X datapoints per graph (usually a number in the range 300-800),
effectively limiting the amount of raw-resolution data you're allowed to see at
once (the system will automatically switch to courser resolution when you
stretch the X-axis). I completely understand the impulse behind this but I
personally find that impulse more than a little patronizing. The cynic in me
suspects systems that do this are choosing responsive UI over data resolution
to protect the help-desk from complaints about slow graphs, and as an end-user
who is using the system in a problem solving capacity, I feel that's a mistake
I should be allowed make when I need to. If you're considering one of these
systems, ask if you can click somewhere to opt-out of the cap.

### Auto-discovery
Beware tools that tout auto-discovery as a feature. If you don't know how many
systems you have, how they are attached to the network, and which ones have
databases and which ones have web servers, you should be shopping for a
configuration management system before you choose your monitoring system.
Network-scanning autodiscovery features inside monitoring systems (which
honestly never worked very well anyway) are only going to make your life more
complicated. Every type of infrastructure maintenance software you can imagine
exists in one form or another today (I recommend saltstack), so adopt something
and use it to roll out your monitoring agents and maintain state across your
network. 

I should note I'm not talking about the autodiscovery used by APM systems like
Appdynamics and Ruxit who are trying to draw context from your infrastructure
that can be used as input to some larger machine-learning computation. If you
find that stuff helpful run with it, but yes, aside from those systems who have
loftier goals of their own, be careful with monitoring tools that are still
excited about autodiscovery. Make sure they're not actually just accidently
confessing their lack of configuration-management support. Think about your
configuration management strategy and how this monitoring tool might fit into
it, and run away if it feels like the tool might impede or otherwise complicate
your ability to centrally manage the software on your servers.

I should also note that autodiscovery still has a place in the context of
devices you can't manage in a configuration management system, like switches,
routers, and various other network devices. 

