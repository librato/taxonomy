mentions: static thresholds, push v pull, data summarization

## A Few things you should know about monitoring systems

## Smaller is probably better
Sometimes it seems to me that everyone who has ever started a new monitoring
tool said to themselves "THIS tool will be able to monitor ANYTHING with ZERO
EFFORT!". The probably counter-intuitive truth is that monitoring tools excel
when they start with a very strong focus and iterate on it until it's
rock-solid.  This book exists today to more or less deliver this message:
beware the tool that claims to do it all, because today, as I write this, the
reality is there probaby isn't one tool that's going to take care of everything
for you. It's really not a question of features, it's a question of trade-offs. 

Simple tools often have scaling problems. But then again, tools that scale well
are often difficult to maintain.  Tools that are easy to maintain are often
expensive, and cheaper tools tend to become more widely adopted throughout the
organization, and on and on. The combination of tools you'll eventually choose
should have way more to do with the people around you, and things like how much
money and time you have, than how many thingies the monitoring system claims to
be able to watch.

But even if that weren't true -- if we could ignore the human factors invovled
here, there certainly is no single monitoring tool that has this entire problem
solved, and that's just a laws of physics thing. As you'll see below, if you
need raw-resolution data, you're going to have to store less of it, or foot the
bill for a large stream-processing and storage infrastructure. If you want
flexibility with respect to *how* a tool monitors the things you care about,
you'll have to develop some in-house expertiese to maintain your
customizations. That's just the way it is. As my Mom used to say: Sorry, them's
the breaks kid.

So if you find yourself considering a tool that claims to be the final and
ultimate solution to the monitoring problem, my advice is to assume you're
considering a tool (or more likely a salesperson) that doesn't understand the
problem very well and move on.  You want to be using tools that are savvy to,
and up-front about their limitations.

## Push vs Pull
## Data summarization and storage

## Auto-discovery
Beware tools that tout auto-discovery as a feature. If you don't know how many
systems you have, how they are attached to the network, and which ones have
databases and which ones have web servers, you have bigger problems than
monitoring. Configuration management has more than killed the need for
network-scanning autodiscovery features inside monitoring systems (which
honestly never worked very well anyway). Every type of infrastructure
maintenance software you can imagine exists in one form or another today, so
adopt something and use it to roll out your monitoring agents and maintain
state across your network. 

I should note I'm not talking about the autodiscovery used by APM systems like
Appdynamics and Ruxit who are trying to draw context from your infrastructure
that can be used as input to some larger machine-learning computation. If you
find that stuff helpful run with it, but yes, aside from those systems who have
loftier goals of their own, be careful with monitoring tools that are still
excited about autodiscovery. Make sure they're not actually just accidently
confessing their lack of configuration-management support. Think about your
configuration management strategy and how this monitoring tool might fit into
it, and run away if it feels like the tool might impede or otherwise complicate
your ability to centrally manage the software on your servers.

## A Few types of monitoring systems

### APM
The problem with distributed web architectures is that they're so...
distributed. Your code is slathered from inside the end-users browser all the
way back to the Database (which you've heard is somewhere in Pittsburg).
Application Performance Monitoring, or APM tries to measure the performance
characteristics of a web-application from one end to the other; breaking down
how long every little hunk of code took to do it's thing, so when someone says
"the website is slow" you can hopefully see where you need to go to fix it.

Typically these tools use Bytecode injection and/or Monkey-patching to modify
your code, compiler or interpretor at run-time, wrapping classes and functions
with versions that extract timing information.  Those timing numbers are then
emitted as metrics and sent into the APM's data collection framework. Data
resolution and retention varies widely, though many of these tools work on a
60-second tick, which is perfectly adequate in real life.

In my admittedly limited experience (and no-doubt already tiresome opinion),
APM tools remain effective by retaining their focus on augmenting engineering
know-how and otherwise staying out of the way. Many do this well, and many
others sprawl here and there in an attempt to implement machine learning or
draw world-maps and node/edge diagrams. Don't get me wrong, many of these
features are nifty, and helpful for non-technical users. That's great, but
beware tools that put that stuff first, actually making you click through maps
to get to timing data or otherwise attempting to enforce their interpretation
of your architecture in the UI in ways that make you work around it. You're
going to want simple line-graphs of timing data way more often than you're
going to want a map, and you're going to be using the tool more often than
non-technical users (assuming it actually works).

### RUM
RUM, or "Real User Monitoring" used to be a thing before it was swallowed by
APM. The idea is, since you have javascript running inside your user's browser,
you may as well inject a timer or two, just to see how long it took that user
to load your page. RUM is probably the biggest deal in web-performance
monitoring in the last decade or so. It's really helping people who want to
make things faster close the loop with people who want things to be faster. 

There aren't very many RUM-only tools out there (none are represented in this
book) because RUM has largly become a feature of the various APM systems. Bucky
(http://github.hubspot.com/bucky/) is a good open source library if you're just
looking for some stand-alone RUM.

### Exception Tracking

Exception tracking systems count crashes (In polite society we refer to them as
"exceptions"). They usually do this from within web-based applications
server-side, or in code running in your user's browser on the client-side. They
often walk the line between RUM and Log-Processing systems, having neither the
open-endedness of logging systems nor the performance-centric focus of RUM.
Their simplicity and single-mindedness make them cheap, easy to configure,
simple to use, and very reliable. A collection of happy factors that combine to
often make them the go-to application monitoring solution when centralized
logging is too much of a hassle, and APM/RUM is too hulking, confusing,
unreliable and/or expensive. 

These tools generally rely on the exception handling charateristics built-in to
languages like Java and Python; giving you a function to call when exceptions
happen or a function to wrap dangerous code in for languages that implement
error handeling instead. Since the data is a-periodic (hopefully your code only
crashes so often) and finite (there are only so many ways it can crash), these
systems almost always store raw-resolution data to relational data-stores, and
boast simple, effective UI's. Beware that because browser quality varies so
drastically, the data you get from these tools browser-side will also very
drastrically. 

### Remote Polling
