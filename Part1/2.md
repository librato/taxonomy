# Re: Monitoring

## A Few types of monitoring systems
Throughout the taxonomy I'll refer to this or that tool being a "centralized
poller" or an "APM" tool and etc.. so lets get some jargon straight before we
move ahead.

### Centralized pollers
The oldest tools we describe as "monitoring tools" are centralized pollers.
They are usually single hosts that sits somewhere on the network and, like the
unfortunate employee in those cell-provider commercials, constantly asks the
question "can you hear me now?" to every host you configure it to "monitor".
If any of those hosts fail to respond, the system sends you a notification
(typically via SMS, but more commonly these days, via an alerting service like
PagerDuty or VictorOps). 

Centralized pollers notoriously scale pretty terribly, and also usually require
a lot of up-front static configuration from you. They also typically work on a
one-minute or greater "tick". 

### Passive collectors 
More modern tools often sit passively on the network and wait for updates from
remote agents. Passive collectors always employ agents, and they generally
scale better than Centralized pollers. Many systems also tow the line, using
both active polling and passive collection depending on how you configure them.

### Roll-up Collectors 
Roll-up patterns are often used in the HPC world where we have thousands of
individual on-premise hosts. The idea is to use a series of systematic roll-ups
between systems to minimize the polling load. Imagine a rack of servers that
all collect and send their metrics to the server at the top of their rack).

Roll-up systems can collect metrics on the order of seconds from a massive
number of individual hosts. A good example of this pattern in the wild is
Ganglia, whose Gmond agent uses a multicast gossip protocol to detect and
aggregate metrics between systems in the same cluster.

### Process Emitters/Reporters
In this book, when I say the word "instrumentation" I'm speaking of code that
resides inside the process (literally inside the PID) of the thing you want to
monitor. These are sometimes called "monitoring clients". Instrumentation
libraries enable software developers to make measurements from inside their
applications, counting the number of times this or that function is involked
and timing interactions with things like database services and etc..

Instrumentation libraries typically use one of two patterns internally.
Emitters immediatly purge their metrics via a non-blocking channel (usually
inter-process or UDP to a locally listening socket), while Reporters use a
non-blocking thread to hold their metrics in-memory, reporting them only when
asked (usually via a listening network socket. Statsd for example is a wildly
popular process-emitter while Dropwizard metrics is a great process-reporter. 

### APM
The problem with distributed web architectures is that they're so...
distributed. Application Performance Monitoring, or APM tries to measure the
performance characteristics of a web-application from one end to the other;
breaking down how long every little hunk of code took to do it's thing, so when
someone says "the website is slow" you can hopefully see where you need to go
to fix it.

Typically these tools use Bytecode injection and/or Monkey-patching to modify
your code, compiler or interpretor at run-time, wrapping classes and functions
with versions that extract timing information.  Those timing numbers are then
emitted as metrics and sent into the APM's data collection framework. Data
resolution and retention varies widely, though many of these tools work on a
60-second tick, which is perfectly adequate in real life.

### RUM
RUM, or "Real User Monitoring" used to be a thing before it was swallowed by
APM. The idea is, since you have javascript running inside your user's browser,
you may as well inject a timer or two, just to see how long it took that user
to load your page. RUM is probably the biggest deal in web-performance
monitoring in the last decade or so. It's really helping people who want pages
to load faster close the loop with the people who are working to make those
pages load faster.

There aren't very many RUM-only tools out there (none are represented in this
book) because RUM has largly become a feature of the various APM systems. Bucky
(http://github.hubspot.com/bucky/) is a good open source library if you're just
looking for some stand-alone RUM.

### Exception Tracking

Exception tracking systems count crashes (In polite society we refer to them as
"exceptions"). They usually do this from within web-based applications
server-side, or in code running in your user's browser on the client-side. They
often walk the line between RUM and Log-Processing systems, having neither the
open-endedness of logging systems nor the performance-centric focus of RUM.
Their simplicity and single-mindedness make them cheap, easy to configure,
simple to use, and very reliable. A collection of happy factors that combine to
often make them the go-to application monitoring solution when centralized
logging is too much of a hassle, and APM/RUM is too hulking, confusing,
unreliable and/or expensive. 

### Remote Polling

Remote pollers use a geographically distributed array of "sensors" (machines
capable of running /bin/ping (I jest (they also run telnet)) to collect
availability data, and sometimes some light performance data from a publicly
facing service like your website or mail server. Many APM tools and commercial
monoliths (like Circonus) have their own remote polling networks which they
operate along-side their other offerings, but there are also plenty of
commercial offerings that specialize in remote polling as a service (like
Pingdom, and Nodeping).

## A Few things you should know about monitoring systems
While we're here, I may as well give you some unsolicited advice. Consider what
follows to be just one person's extremely well informed and probably objectively
correct opinion: 

### Think big, but use small tools.
New monitoring systems spring into being for myriad reasons. The best ones come
from engineers like you who couldn't find the tool they needed to get their
work done. These tools tend to be small, efficient, and purpose-specific. They
tend to get along well with other tools, and they solve more problems than they
create.  Other tools don't do these things. They come from people who were
looking for a market niche who say things like "THIS tool can monitor ANYTHING
with ZERO EFFORT!".  Monitoring tools excel when they start with a very strong
focus and iterate on it until it's rock-solid. Don't be afraid to use more than
one, and chose tools that lend themsleves to being wired up to other tools. 

### Push vs Pull

This is perhaps the monitoring world's analog to Emacs vs Vi, and like that
argument, it was never very interesting to begin with. Pull-based-tools have to
work harder, so they don't scale as well (see centralized-collectors above).
Push-based tools just sit around and wait for stuff to happen, so they're
easier to scale. Yep, that's pretty much it, there's no real reason to get hung
up on it, each technique has it's place. When you encounter a pull-based
system, make sure to ask yourself if that might become a problem later and
choose accordingly. 

### Agent vs Agentless
This is another of the classic debates. In the 90's, all monitoring agents were
horrible pieces of software that wreaked all sorts of havock (a few still are
today). Back then, agents were to be avoided at all costs, but these days
monitoring agents are generally pretty well behaved. The systems that use a
good agent are generally to be preferred over agentless systems, which always
need to commit some egregious security no-no's like running under domain-admin
priviledges or centrally storing general-purpose credentials to all your boxes.

Collectd is a solid, efficient, well-tested open-source monitoring agent upon
which many commercial products base their proprietary collector software.
Beware of agentless systems (make sure you understand the security trade-offs
they make), and take it as a good sign if the vendor you're looking at has a
collectd-based agent.

### Data summarization and storage
Time series databases are really hard. Anyone who says otherwise is trying to
sell you something. The main problem is that raw-resolution metrics take up too
much space on disk. Over time, they build up until your quieries are too slow
and your disks are full. 

To get around this, most TSDB's automatically  perform some form of data
summarization (also referred to as aggregation or rollups). The idea is to
consolidate the individual data points you collect into fewer datapoints that
accurately summarize the original measurements.

I don't have the room to devle very deeply into this subject but suffice to say
that monitoring systems vary widely in how well they implement data
aggregation. That's relevent to you because you want to ensure that your data
isn't destroyed in the summarization process. On one extreme are the systems
that expose all of the dials and knobs to you, giving you all the rope you need
to hang yourself and walking away, while on the other are the systems which try
to insulate you by blithy destroying your data for you and assuming you're too
stupid or apathetic to care.

Good metrics systems are very clear about their data retention and resolution
schedules. They'll say "we store 1-second resolution data for X days, and then
we agregate it by doing Y". Be careful with systems that don't provide this
information, because they're probably averaging your data into oblivion without
telling you. Many systems will also make you classify your inbound metrics,
assigning to them types like "gauge", "counter", or "timer". It's really
important for you to understand exactly how the system you choose uses those
classifications internally.  Some systems will automatically assign destructive
roll-ups to certain types, always applying mean-average to consolidate gauge
metrics for example.

Finally, several systems employ "data-point capping", which I have mixed
feelings about.  That is, regardless of storage resolution, their UI will only
display X datapoints per graph (usually a number in the range 300-800),
effectively limiting the amount of raw-resolution data you're allowed to see at
once (the system will automatically switch to courser resolution when you
stretch the X-axis). I completely understand the impulse behind this but I
personally find that impulse more than a little patronizing. The cynic in me
suspects systems that do this are choosing responsive UI over data resolution
to protect their own help-desk (or more likely their twitter feed) from
complaints about slow graphs, and as an end-user who is using the system in a
problem solving capacity, I feel that's a mistake I should be allowed make when
I need to. If you're considering one of these systems, ask if you can click
somewhere to opt-out of the cap.

### Auto-discovery
Beware tools that tout auto-discovery as a feature. If you don't know how many
systems you have, how they are attached to the network, and which ones have
databases and which ones have web servers, you should be shopping for a
configuration management system before you choose your monitoring system.
Network-scanning autodiscovery features inside monitoring systems (which
honestly never worked very well anyway) are only going to make your life more
complicated. Every type of infrastructure maintenance software you can imagine
exists in one form or another today (I recommend saltstack), so adopt something
and use it to roll out your monitoring agents and maintain state across your
network. 

I should note that autodiscovery still has a place in the context of devices
you can't manage in a configuration management system, like switches, routers,
and various other network devices. The fancey APM machine learning discovery
protocols are also excluded. 

### Data-to-Ink ratio
Data visualization is a whole field of study unto itself, and although I can't
prove it, I strongly believe that that no discipline ignores it's tennants as
strenuously as the filed of IT monitoing. I really would love to drole on for
pages on this topic but alas every word I write here is one less word I can
write in Part 2, so ;tldr beware monitoring systems where any of the following
are prominently displayed:

* Pie charts
* Doughnut charts
* Speedometers 
* Stoplights
* Node-edge diagrams
* Geographical/Overlay Maps

