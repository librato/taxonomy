# Re: Monitoring

## A Few types of monitoring systems
Throughout the taxonomy I'll refer to this or that tool being a "centralized
poller" or an "APM" tool and etc.. so lets get some jargon straight before we
move ahead.

### Centralized Pollers
The oldest tools we describe as "monitoring tools" are centralized pollers.
They are usually single hosts that sits somewhere on the network and, like the
unfortunate employee in those cell-provider commercials, constantly asks the
question "can you hear me now?" to every host you configure it to "monitor".
If any of those hosts fail to respond, the system sends you a notification
(typically via SMS, but more commonly these days, via an alerting service like
PagerDuty or VictorOps). 

Strict centralized pollers are notoriously difficult to scale and configure.
They also typically work on a one-minute or greater "tick" which limits their
resolution potential in the context of monitoring for performance versus
availability. 

### Passive collectors 
Most modern monitoing systems sit passively on the network and wait for updates
from remote agents. Passive collectors always employ agents, which enables them
to scale better than Centralized pollers. Many systems also tow the line, using
both active polling and passive collection as needs require.

### Roll-up Collectors 
Roll-up patterns are often used in the HPC world where we find thousands of
individual on-premise hosts running CPU or Memory bound processes. The idea is
to use a series of systematic roll-ups between systems to minimize the polling
load. Imagine a rack of servers that all summarize and send their metrics to
the server at the top of their rack).

Roll-up systems can collect metrics on the order of seconds from a massive
number of individual hosts. A good example of this pattern in the wild is
Ganglia, whose Gmond agent uses a multicast gossip protocol to detect and
aggregate metrics between systems in the same cluster.

### Process Emitters/Reporters
In this book, when I use the word "instrumentation" I'm describing code that
resides inside a running process that you want to monitor. Instrumentation
libraries enable software developers to take measurements from inside their
applications, counting the number of times this or that function is invoked and
timing interactions with services like databases and external API's.

Instrumentation libraries typically use one of two patterns internally.
*Emitters* immediately purge their metrics via a non-blocking channel (usually
inter-process or UDP to a locally listening socket), while *Reporters* use a
non-blocking thread to hold their metrics in-memory, reporting them only when
asked (usually via a listening network socket. Statsd for example is a wildly
popular process-emitter while Dropwizard metrics is a great process-reporter. 

### APM
The problem with distributed web architectures is that they're so...
distributed. Application Performance Monitoring, or APM tries to measure the
performance characteristics of a web-application from one end to the other;
breaking down how long every little hunk of code took to do its thing, so when
someone says "the web-site is slow" you can hopefully see where you need to go
to fix it.

Typically these tools use byte code injection and/or Monkey-patching to modify
your code, compiler or interpretor at run-time, wrapping the built-in classes
and functions with modified versions that extract timing information.  Those
timing numbers are then emitted as metrics and sent into the APM's data
collection framework. Data resolution and retention varies widely, though many
of these tools work on a 60-second tick, which is perfectly adequate in real
life.

### RUM
RUM, or "Real User Monitoring" used to be a thing before it was swallowed by
APM. The idea is, since you have javascript running inside your user's browser,
you may as well inject a timer or two, just to see how long it took that user
to load your page. RUM is probably the biggest deal in web-performance
monitoring in the last decade or so. It's a great way to get a real idea of
what your user-experience looks like from your user's perspective.

Although I mention RUM here and there in the taxonomy, there aren't very many
RUM-only tools out there (none are represented in the taxonomy currently). This
is largely because RUM has become a feature of the various APM systems.  Bucky
(http://github.hubspot.com/bucky/) is a good open source library if you're just
looking for some stand-alone RUM.

### Exception Tracking

Exception tracking systems count crashes and panics (In polite society we refer
to them as "exceptions"). They usually do this from within web-based
applications server-side, or in code running in your user's browser on the
client-side. They often walk the line between RUM and Log-Processing systems,
having neither the open-endedness of logging systems nor the
performance-centric focus of RUM.  Their simplicity and single-mindedness make
them cheap, easy to configure, simple to use, and very reliable. A collection
of happy factors that combine to often make them the go-to application
monitoring solution when centralized logging is too much of a hassle, and
APM/RUM is too hulking, confusing, unreliable and/or expensive. 

### Remote Polling

Remote pollers use a geographically distributed array of "sensors" (machines
capable of running /bin/ping (I jest (they also run telnet)) to collect
availability data, and sometimes some light performance data from a publicly
facing service like your web-site or mail server. Many APM tools and commercial
monoliths (like Circonus) have their own remote polling networks which they
operate along-side their other offerings, but there are also plenty of
commercial offerings that specialize in remote polling as a service (like
Pingdom, and Nodeping).

## A Few things you should know about monitoring systems
While we're here, I may as well give you some unsolicited advice. Consider what
follows to be just one person's extremely well informed and probably objectively
correct opinion: 

### Think big, but use small tools.
New monitoring systems spring into being for myriad reasons. The best ones come
from engineers like you who couldn't find the tool they needed to get their
work done. These tools tend to be small, cooperative, and purpose-specific.

Other tools come from people who were looking for a market niche who say things
like "THIS tool can monitor ANYTHING with ZERO EFFORT!".  They are large,
monolithic and sprawly. Monitoring tools excel when they start with a very
strong focus and iterate on it until it's rock-solid. You'll probably need to
use more than one monitoring tool and that's OK. 

### Push vs Pull

This is our analog to Emacs vs Vi, and like that argument, it was never very
interesting to begin with. Pull-based-tools have to work harder, so they don't
scale as well (most are fine into the relm of 7000 hosts or so).  Push-based
tools just sit around and wait for stuff to happen, so they're easier to scale
and they can take measurements more often.  Yep, that's pretty much it, there's
no real reason to get hung up on it, each technique has its place. When you
encounter a pull-based system, make sure to ask yourself if it meets your
scalability and resolution requirements.

### Agent vs Agent-less
This is another of the classic debates. In the 90's, all monitoring agents were
horrible pieces of software that wreaked all sorts of havoc (a few still are
today). Back then, agents were to be avoided at all costs, but these days
monitoring agents are generally pretty well behaved. The systems that use a
good agent are generally to be preferred over agentless systems, which always
need to commit some egregious security no-no's like running under domain-admin
privileges or centrally storing general-purpose credentials to all your boxes.
Beware agentless systems, and make sure you understand the security trade-offs
they make.

### Data summarization and storage
Time series databases are really hard. Anyone who says otherwise is trying to
sell you something. The main problem is that raw-resolution metrics take up too
much space on disk. Over time, they build up until your queries are too slow
and your disks are full. 

To get around this, most TSDB's automatically perform some form of data
summarization (also referred to as aggregation or roll-ups). The idea is to
consolidate the individual data points you collect into fewer data-points that
accurately summarize the original measurements.

I don't have the room to delve very deeply into this subject but suffice to say
that monitoring systems vary widely in how well they implement data
aggregation. That's relevant to you because you want to ensure that your data
isn't destroyed in the summarization process. On one extreme are the systems
that expose all of the dials and knobs to you, giving you all the rope you need
to hang yourself and walking away, while on the other are the systems which try
to insulate you by blithely destroying your data for you and assuming you're too
stupid or apathetic to care.

Good metrics systems are very clear about their data retention and resolution
schedules. They'll say "we store 1-second resolution data for X days, and then
we aggregate it by doing Y". Be careful with systems that don't provide this
information, because they're probably averaging your data into oblivion without
telling you. Many systems will also make you classify your inbound metrics,
assigning to them types like "gauge", "counter", or "timer". These types almost
always directly map to data-summarization schemes internally; always applying
mean-average to consolidate *gauge* metrics for example. Make sure you
understand what your system does with classifications like these.

Finally, several systems employ "data-point capping", which I have mixed
feelings about.  That is, regardless of the underlying storage resolution,
their UI will only display X data-points per graph (usually a number in the
range 300-800). This limits the amount of raw-resolution data you're allowed to
see at once (the system will automatically switch to courser resolution when
you stretch the X-axis). I totally understand the impulse behind this but I
often find it frustating in practice. Also, the cynic in me suspects systems
that do this are choosing responsive UI over data resolution to protect their
own help-desk (or more likely their twitter feed) from complaints about slow
graphs, and as an end-user who is using the system in a problem solving
capacity, I feel that's a mistake I should be allowed make when I need to.
Caveat Emptor

### Auto-discovery
Beware tools that tout auto-discovery as a feature. If you don't know how many
systems you have, how they are attached to the network, and which ones have
databases and which ones have web servers, you should be shopping for a
configuration management system before you choose your monitoring system.
Network-scanning auto-discovery features inside monitoring systems (which
honestly never worked very well anyway) are not helping. They're just enabling
your IT management incompetency. I'm sorry if that sounds harsh, but it's only
because I love you.

I should caveat that auto-discovery still has a place in the context of
embedded devices, which you can't manage in a configuration management system.
The fancy APM machine-learning discovery protocols I also grudgingly exclude.

### Data-to-Ink ratio
Data visualization is a whole field of study unto itself, and although I can't
prove it, I strongly believe that that no discipline ignores its tenants as
strenuously as the filed of IT monitoring. I really would love to drone on for
pages on this topic but alas every word I write here is one less word I can
write in Part 2, so read up on datavis and avoid monitoring systems with pie
and doughnut charts.
